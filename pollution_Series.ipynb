{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyM78t4cEGaXdkMziDWQKY7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyomaland/image_cnns/blob/master/pollution_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isaszuQcivvZ",
        "colab_type": "text"
      },
      "source": [
        "Explanations: https://www.freecodecamp.org/news/forecasting-air-pollution-recurrent-neural-networks/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kbj5HwBFr-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_profiling\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, SimpleRNN\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import model_from_json\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suirbir7Hux6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!wget --no-check-certificate \\\n",
        "    \"http://cmshare.eea.europa.eu/s/6WZZ8dBECmER2EF/download\" \\\n",
        "    -O \"/tmp/pollution_data.zip\"\n",
        "\n",
        "local_zip = '/tmp/pollution_data.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "artIB7XpJfhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local_zip = '/tmp/pollution_data.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5-sRoZSILDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_dir = \"/tmp/\"\n",
        "date_vars = ['DatetimeBegin','DatetimeEnd']\n",
        "\n",
        "agg_ts = pd.read_csv(project_dir + '/BE_1_2013-2015_aggregated_timeseries.csv', sep='\\t', parse_dates=date_vars, date_parser=pd.to_datetime)\n",
        "meta = pd.read_csv(project_dir + '/BE_2013-2015_metadata.csv', sep='\\t')\n",
        "\n",
        "print(f'aggregated timeseries shape:{agg_ts.shape}')\n",
        "print(f'metadata shape:{meta.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CRbML4KQFxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agg_ts.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylGiabaXNaeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "plt.plot(agg_ts.groupby('DatetimeBegin').count(), 'o', color='skyblue')\n",
        "plt.title('Nb of measurements per DatetimeBegin')\n",
        "plt.ylabel('number of measurements')\n",
        "plt.xlabel('DatetimeBegin')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ItEYN9P6rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(agg_ts.DatetimeBegin.value_counts()[:5])\n",
        "print(agg_ts.DatetimeEnd.value_counts()[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBA2Opd8R8V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ser_avail_days = agg_ts.groupby('SamplingPoint').nunique()['DatetimeBegin']\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.hist(ser_avail_days.sort_values(ascending=False))\n",
        "plt.ylabel('Nb SamplingPoints')\n",
        "plt.xlabel('Nb of Unique DatetimeBegin')\n",
        "plt.title('Distribution of Samplingpoints by the Nb of available measurement days')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY1G3mORSoRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = agg_ts.loc[agg_ts.DataAggregationProcess=='P1D', :] \n",
        "df = df.loc[df.UnitOfAirPollutionLevel!='count', :]\n",
        "df = df.loc[df.SamplingPoint.isin(ser_avail_days[ser_avail_days.values >= 1000].index), :]\n",
        "vars_to_drop = ['AirPollutant','AirPollutantCode','Countrycode','Namespace','TimeCoverage','Validity','Verification','AirQualityStation',\n",
        "               'AirQualityStationEoICode','DataAggregationProcess','UnitOfAirPollutionLevel', 'DatetimeEnd', 'AirQualityNetwork',\n",
        "               'DataCapture', 'DataCoverage']\n",
        "df.drop(columns=vars_to_drop, axis='columns', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp033dqpT-k_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dates = list(pd.period_range(min(df.DatetimeBegin), max(df.DatetimeBegin), freq='D').values)\n",
        "samplingpoints = list(df.SamplingPoint.unique())\n",
        "\n",
        "new_idx = []\n",
        "for sp in samplingpoints:\n",
        "    for d in dates:\n",
        "        new_idx.append((sp, np.datetime64(d)))\n",
        "\n",
        "df.set_index(keys=['SamplingPoint', 'DatetimeBegin'], inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "df = df.reindex(new_idx)\n",
        "#print(df.loc['SPO-BETR223_00001_100','2013-01-29'])  # should contain NaN for the columns\n",
        "\n",
        "df['AirPollutionLevel'] = df.groupby(level=0).AirPollutionLevel.bfill().fillna(0)\n",
        "#print(df.loc['SPO-BETR223_00001_100','2013-01-29'])  # NaN are replaced by values of 2013-01-30\n",
        "print('{} missing values'.format(df.isnull().sum().sum()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Hj1MSoVAdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.loc['SPO-BETR223_00001_100',:] #just for one point"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SneHAZ17VgBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df.query('DatetimeBegin < \"2014-07-01\"')\n",
        "valid = df.query('DatetimeBegin >= \"2014-07-01\" and DatetimeBegin < \"2015-01-01\"')\n",
        "test = df.query('DatetimeBegin >= \"2015-01-01\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMUlN9YlV3MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save column names and indices to use when storing as csv\n",
        "cols = train.columns\n",
        "train_idx = train.index\n",
        "valid_idx = valid.index\n",
        "test_idx = test.index\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train = scaler.fit_transform(train)\n",
        "valid = scaler.transform(valid)\n",
        "test = scaler.transform(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVwC0c8BV8Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame(train, columns=cols, index=train_idx)\n",
        "valid = pd.DataFrame(valid, columns=cols, index=valid_idx)\n",
        "test = pd.DataFrame(test, columns=cols, index=test_idx)\n",
        "\n",
        "train.to_csv('/tmp/train.csv')\n",
        "valid.to_csv('/tmp/valid.csv')\n",
        "test.to_csv('/tmp/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPQgx4eWW2ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/tmp/train.csv', header=0, index_col=0).values.astype('float32')\n",
        "valid = pd.read_csv('/tmp/valid.csv', header=0, index_col=0).values.astype('float32')\n",
        "test = pd.read_csv('/tmp/test.csv', header=0, index_col=0).values.astype('float32')\n",
        "\n",
        "def plot_loss(history, title):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(history.history['loss'], label='Train')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Nb Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    val_loss = history.history['val_loss']\n",
        "    min_idx = np.argmin(val_loss)\n",
        "    min_val_loss = val_loss[min_idx]\n",
        "    print('Minimum validation loss of {} reached at epoch {}'.format(min_val_loss, min_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSlaQieuX-zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_lag = 14\n",
        "\n",
        "train_data_gen = TimeseriesGenerator(train, train, length=n_lag, sampling_rate=1, stride=1, batch_size = 5)\n",
        "valid_data_gen = TimeseriesGenerator(valid, valid, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)\n",
        "test_data_gen = TimeseriesGenerator(test, test, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDwqmYFTY75U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_rnn = Sequential()\n",
        "simple_rnn.add(SimpleRNN(4, input_shape=(n_lag, 1),return_sequences=True))\n",
        "simple_rnn.add(SimpleRNN(8))\n",
        "simple_rnn.add(Dense(1))\n",
        "simple_rnn.compile(loss='mae', optimizer=RMSprop())\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/tmp/simple_rnn_weights.hdf5'\n",
        "                               , verbose=0\n",
        "                               , save_best_only=True)\n",
        "earlystopper = EarlyStopping(monitor='val_loss'\n",
        "                             , patience=10\n",
        "                             , verbose=0)\n",
        "with open(\"/tmp/simple_rnn.json\", \"w\") as m:\n",
        "    m.write(simple_rnn.to_json())\n",
        "\n",
        "simple_rnn_history = simple_rnn.fit(train_data_gen\n",
        "                                              , epochs=100\n",
        "                                              , validation_data=valid_data_gen\n",
        "                                              , verbose=1\n",
        "                                              , callbacks=[checkpointer, earlystopper])\n",
        "plot_loss(simple_rnn_history, 'SimpleRNN - Train & Validation Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hCDmljzbtYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Simple LSTM\n",
        "simple_lstm = Sequential()\n",
        "simple_lstm.add(LSTM(4, input_shape=(n_lag, 1)))\n",
        "simple_lstm.add(Dense(1))\n",
        "simple_lstm.compile(loss='mae', optimizer=RMSprop())\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/tmp/simple_lstm_weights.hdf5'\n",
        "                               , verbose=0\n",
        "                               , save_best_only=True)\n",
        "earlystopper = EarlyStopping(monitor='val_loss'\n",
        "                             , patience=10\n",
        "                             , verbose=0)\n",
        "with open(\"/tmp/simple_lstm.json\", \"w\") as m:\n",
        "    m.write(simple_lstm.to_json())\n",
        "\n",
        "simple_lstm_history = simple_lstm.fit_generator(train_data_gen\n",
        "                                                , epochs=100\n",
        "                                                , validation_data=valid_data_gen\n",
        "                                                , verbose=1\n",
        "                                                , callbacks=[checkpointer, earlystopper])\n",
        "plot_loss(simple_lstm_history, 'Simple LSTM - Train & Validation Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GFch-QDccvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stacked_lstm = Sequential()\n",
        "stacked_lstm.add(LSTM(16, input_shape=(n_lag, 1), return_sequences=True))\n",
        "stacked_lstm.add(LSTM(32, return_sequences=True))\n",
        "stacked_lstm.add(LSTM(64))\n",
        "stacked_lstm.add(Dense(1))\n",
        "stacked_lstm.compile(loss='mae', optimizer=RMSprop())\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/tmp/stacked_lstm_weights.hdf5'\n",
        "                               , verbose=0\n",
        "                               , save_best_only=True)\n",
        "earlystopper = EarlyStopping(monitor='val_loss'\n",
        "                             , patience=10\n",
        "                             , verbose=0)\n",
        "with open(\"/tmp/stacked_lstm.json\", \"w\") as m:\n",
        "    m.write(stacked_lstm.to_json())\n",
        "\n",
        "stacked_lstm_history = stacked_lstm.fit(train_data_gen\n",
        "                                                  , epochs=100\n",
        "                                                  , validation_data=valid_data_gen\n",
        "                                                  , verbose=1\n",
        "                                                  , callbacks=[checkpointer, earlystopper])\n",
        "plot_loss(stacked_lstm_history, 'Stacked LSTM - Train & Validation Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPrNAbJJgcZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_best_model(model):\n",
        "    # Load model architecture from JSON\n",
        "    model_architecture = open('/tmp/'+model+'.json', 'r')\n",
        "    best_model = model_from_json(model_architecture.read())\n",
        "    model_architecture.close()\n",
        "    # Load best model's weights\n",
        "    best_model.load_weights('/tmp/'+model+'_weights.hdf5')\n",
        "    # Compile the best model\n",
        "    best_model.compile(loss='mae', optimizer=RMSprop())\n",
        "    # Evaluate on test data\n",
        "    perf_best_model = best_model.evaluate_generator(test_data_gen)\n",
        "    print('Loss on test data for {} : {}'.format(model, perf_best_model))\n",
        "\n",
        "eval_best_model('simple_rnn')\n",
        "eval_best_model('simple_lstm')\n",
        "eval_best_model('stacked_lstm')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}